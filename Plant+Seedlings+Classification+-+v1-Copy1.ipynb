{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "from skimage import exposure\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataAugment = True\n",
    "if(dataAugment): from keras.preprocessing.image import ImageDataGenerator\n",
    "    \n",
    "augRatio = 2  #Data augmentation時，要產生幾倍數量的dataset\n",
    "testimg = \"dataset/train/Small-flowered Cranesbill/1f14ed265.png\"\n",
    "ratioVal = 0.2  #要從train dataset取出多少比例的資料作為validate dataset\n",
    "reSize=(28,28)  #訓練前圖片要縮放的尺寸\n",
    "epochs = 100  #要跑幾個世代\n",
    "batch_size = 64   #每批次提取多少數量的圖片進行訓練\n",
    "trainDataset = \"dataset/train\"  #Train dataset的路徑\n",
    "testDataset = \"dataset/test\"   #Test dataset的路徑\n",
    "#Lable的對應, 每個Lable對應到一個數值\n",
    "dict_labels = {\"Black-grass\": 0,\"Charlock\": 1, \"Cleavers\": 2, \"Common Chickweed\": 3, \"Common wheat\": 4, \"Fat Hen\": 5,\n",
    "              \"Loose Silky-bent\": 6, \"Maize\": 7, \"Scentless Mayweed\": 8, \"Shepherds Purse\": 9, \"Small-flowered Cranesbill\": 10, \n",
    "              \"Sugar beet\": 11 }\n",
    "\n",
    "#存放dataset及Label使用\n",
    "images = []\n",
    "labels = []\n",
    "labels_hot = []\n",
    "testImages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_train_history(train_history, train, validation):\n",
    "    plt.plot(train_history.history[train])\n",
    "    plt.plot(train_history.history[validation])\n",
    "    plt.title('Train history')\n",
    "    plt.ylabel(train)\n",
    "    plt.xlabel('Epoch')\n",
    "    legendLoc = 'lower right' if(train=='acc') else 'upper right'\n",
    "    plt.legend(['train', 'validation'], loc=legendLoc)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image(folder):    \n",
    "    global images, labels, labels_hot, dict_labels\n",
    "    \n",
    "    for filename in os.listdir(folder):\n",
    "        label = os.path.basename(folders)\n",
    "        className = np.asarray( label )        \n",
    "        if label is not None:\n",
    "            labels.append(className)\n",
    "            labels_hot.append(dict_labels[label])\n",
    "            #np.append(labels, className , axis=0)\n",
    "            #np.append(labels_hot, np.array(dict_labels[label]), axis=0)\n",
    "            \n",
    "        img = cv2.imread(os.path.join(folder,filename))\n",
    "        if img is not None:            \n",
    "            img = extractPlantsArea(img)\n",
    "            img = cv2.resize(img,reSize,interpolation=cv2.INTER_CUBIC)\n",
    "            images.append(np.array(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''    \n",
    "def extractPlantsArea(orgimg):\n",
    "    img = cv2.cvtColor(orgimg, cv2.COLOR_BGR2HSV)\n",
    "    lower_color = np.array([25, 40, 40]) \n",
    "    upper_color = np.array([100, 225, 225]) \n",
    "    plants = cv2.inRange(img, lower_color , upper_color)\n",
    "            \n",
    "    clone = plants.copy()    \n",
    "    _, cnts, _ = cv2.findContours(clone,cv2.RETR_EXTERNAL ,cv2.CHAIN_APPROX_SIMPLE)          \n",
    "    mask = np.zeros(orgimg.shape[:2], dtype=\"uint8\")\n",
    "    \n",
    "    for c in cnts:\n",
    "        cv2.drawContours(mask, [c], -1, 255, -1)\n",
    "\n",
    "    image = cv2.bitwise_and(orgimg, orgimg, mask=mask)\n",
    "    \n",
    "    #img = cv2.bilateralFilter(orgimg,3,255,255) \n",
    "    image_blur = cv2.GaussianBlur(image, (0, 0), 3)\n",
    "    img = cv2.addWeighted(image, 1.5, image_blur, -0.5, 0)\n",
    "    \n",
    "    return img\n",
    "'''        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def extractPlantsArea(orgimg):\n",
    "    img = cv2.bilateralFilter(orgimg,3,255,255)     \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    lower_color = np.array([25, 40, 40]) \n",
    "    upper_color = np.array([100, 225, 225]) \n",
    "    plants = cv2.inRange(img, lower_color , upper_color)\n",
    "            \n",
    "    clone = plants.copy()    \n",
    "    _, cnts, _ = cv2.findContours(clone,cv2.RETR_EXTERNAL ,cv2.CHAIN_APPROX_SIMPLE)          \n",
    "    mask = np.zeros(orgimg.shape[:2], dtype=\"uint8\")\n",
    "    \n",
    "    for c in cnts:\n",
    "        cv2.drawContours(mask, [c], -1, 255, -1)\n",
    "\n",
    "    image = cv2.bitwise_and(orgimg, orgimg, mask=mask)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load images from Train dataset\n",
      "Load dataset/train/Common Chickweed ...\n",
      "Load dataset/train/Scentless Mayweed ...\n",
      "Load dataset/train/Maize ...\n",
      "Load dataset/train/Charlock ...\n",
      "Load dataset/train/Loose Silky-bent ...\n",
      "Load dataset/train/Common wheat ...\n",
      "Load dataset/train/Cleavers ...\n",
      "Load dataset/train/Sugar beet ...\n",
      "Load dataset/train/Small-flowered Cranesbill ...\n",
      "Load dataset/train/Fat Hen ...\n",
      "Load dataset/train/Shepherds Purse ...\n",
      "Load dataset/train/Black-grass ...\n",
      "Load images from Test dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"Load images from Train dataset\")\n",
    "for folders in glob.glob(trainDataset+\"/*\"):\n",
    "    print(\"Load {} ...\".format(folders))\n",
    "    load_image(folders)\n",
    "    \n",
    "print(\"Load images from Test dataset\")    \n",
    "for filename in os.listdir(testDataset):\n",
    "    img = cv2.imread(testDataset+\"/\"+filename)\n",
    "    \n",
    "    if img is not None:\n",
    "        img = extractPlantsArea(img)\n",
    "        img = cv2.resize(img,reSize,interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        testImages.append(np.array(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape=(4750, 28, 28, 3) , labels_hot.shape==(4750,)\n",
      "[3 3 3 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "images = np.array(images) \n",
    "labels_hot = np.array(labels_hot)\n",
    "testImages = np.array(testImages) \n",
    "print(\"images.shape={} , labels_hot.shape=={}\".format(images.shape, labels_hot.shape))\n",
    "print(labels_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:Common Chickweed , ID:3, shape:(28, 28, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA0ERfiAowg8ERfiBoAg/ENT/Ax8YX7VYHTcHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fef73760ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampleID = 222\n",
    "#print(images[3])\n",
    "plt.imshow(images[sampleID])\n",
    "print(\"Label:{} , ID:{}, shape:{}\".format(labels[sampleID], labels_hot[sampleID], images[sampleID].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if(dataAugment):\n",
    "    (trainData, valiData, trainLabels, valiLabels) = train_test_split(images, labels_hot, test_size=ratioVal, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainData records: 3800\n",
      "trainData.shape=(3800, 28, 28, 3) trainLabels.shape=(3800,)\n",
      "valiData records: 950\n",
      "valiData.shape=(950, 28, 28, 3) valiLabels.shape=(950,)\n"
     ]
    }
   ],
   "source": [
    "print(\"trainData records: {}\".format(len(trainData)))\n",
    "print(\"trainData.shape={} trainLabels.shape={}\".format(trainData.shape, trainLabels.shape))\n",
    "\n",
    "if(dataAugment):\n",
    "    print(\"valiData records: {}\".format(len(valiData)))\n",
    "    print(\"valiData.shape={} valiLabels.shape={}\".format(valiData.shape, valiLabels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset:  (3800, 12)\n",
      "Validate dataset: (950, 12)\n"
     ]
    }
   ],
   "source": [
    "trainLabels_hot = np_utils.to_categorical(trainLabels)\n",
    "print(\"Train dataset: \", trainLabels_hot.shape)\n",
    "\n",
    "if(dataAugment):\n",
    "    valiLabels_hot = np_utils.to_categorical(valiLabels)\n",
    "    print(\"Validate dataset:\", valiLabels_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData_normalize = trainData.astype('float32') / 255.0\n",
    "\n",
    "if(dataAugment):\n",
    "    valiData_normalize = valiData.astype('float32') / 255.0\n",
    "    \n",
    "testImages_normalize = testImages.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if(dataAugment):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        zoom_range = 0.08,\n",
    "        height_shift_range = 0.08,\n",
    "        width_shift_range = 0.08,\n",
    "        shear_range=0.3,\n",
    "        rotation_range = 8)\n",
    "    \n",
    "    valdi_datagen =  ImageDataGenerator(\n",
    "        zoom_range = 0.08,\n",
    "        height_shift_range = 0.08,\n",
    "        width_shift_range = 0.08,\n",
    "        shear_range=0.3,\n",
    "        rotation_range = 8)\n",
    "    \n",
    "    train_generator = train_datagen.flow(trainData_normalize, trainLabels_hot, batch_size=batch_size)\n",
    "    valdi_generator = valdi_datagen.flow(valiData_normalize, valiLabels_hot, batch_size=batch_size)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "#from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        2432      \n",
      "_________________________________________________________________\n",
      "p_re_lu_1 (PReLU)            (None, 28, 28, 32)        25088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "p_re_lu_2 (PReLU)            (None, 14, 14, 64)        12544     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 3136)              12544     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              3212288   \n",
      "_________________________________________________________________\n",
      "p_re_lu_3 (PReLU)            (None, 1024)              1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                12300     \n",
      "=================================================================\n",
      "Total params: 3,297,100\n",
      "Trainable params: 3,290,636\n",
      "Non-trainable params: 6,464\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(5, 5), padding='same', input_shape=(reSize[0], reSize[1], 3)))\n",
    "#model.add(LeakyReLU(alpha=.001))   # add an advanced activation\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(rate=0.25))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(LeakyReLU(alpha=.001))   # add an advanced activation\n",
    "model.add(PReLU())\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(1024))\n",
    "#model.add(LeakyReLU(alpha=.001))   # add an advanced activation\n",
    "model.add(PReLU())\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(len(dict_labels), activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom keras.optimizers import SGD\\nfrom keras.models import Model\\nfrom keras.layers import Input\\n\\nimg_input = Input(shape=(reSize[0], reSize[1], 3))\\n# Block 1\\nx = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\\nx = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\\nx = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\\n \\n# Block 2\\nx = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\\nx = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\\nx = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\\n \\n# Block 3\\nx = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\\nx = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\\nx = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\\nx = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\\n \\n# Block 4\\nx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\\nx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\\nx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\\nx = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\\n \\n# Block 5\\nx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\\nx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\\nx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\\nx = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\\n \\nx = Flatten(name='flatten')(x)\\nx = Dense(4096, activation='relu', name='fc1')(x)\\nx = Dense(4096, activation='relu', name='fc2')(x)\\n#x = Dense(12, activation='softmax', name='predictions')(x)\\npredictions = Dense(12, activation='softmax')(x)\\n\\nmodel = Model(inputs=img_input, outputs=predictions)\\n\""
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "\n",
    "img_input = Input(shape=(reSize[0], reSize[1], 3))\n",
    "# Block 1\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    " \n",
    "# Block 2\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    " \n",
    "# Block 3\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    " \n",
    "# Block 4\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    " \n",
    "# Block 5\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    " \n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "#x = Dense(12, activation='softmax', name='predictions')(x)\n",
    "predictions = Dense(12, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=img_input, outputs=predictions)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom keras.applications.vgg16 import VGG16\\nfrom keras.preprocessing import image\\nfrom keras.applications.vgg16 import preprocess_input\\nfrom keras.models import Model\\n\\nbase_model = VGG16(weights=None, include_top=True, input_shape=trainData_normalize.shape[1:], classes=12)\\nbase_model.compile(optimizer=SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True), loss='categorical_crossentropy', metrics=['accuracy'])   \\nbase_model.fit(trainData_normalize, trainLabels_hot, nb_epoch=10, batch_size=256, verbose=1)\\nbase_model.evaluate(valiData_normalize, valiLabels_hot, batch_size=256, verbose=1)\\n\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    "\n",
    "base_model = VGG16(weights=None, include_top=True, input_shape=trainData_normalize.shape[1:], classes=12)\n",
    "base_model.compile(optimizer=SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True), loss='categorical_crossentropy', metrics=['accuracy'])   \n",
    "base_model.fit(trainData_normalize, trainLabels_hot, nb_epoch=10, batch_size=256, verbose=1)\n",
    "base_model.evaluate(valiData_normalize, valiLabels_hot, batch_size=256, verbose=1)\n",
    "'''\n",
    "\n",
    "#model = Model(inputs=img_input, outputs=predictions)\n",
    "#model.compile(optimizer='rmsprop',\n",
    "#              loss='categorical_crossentropy',\n",
    "#              metrics=['accuracy'])\n",
    "#model.fit(trainData_normalize, trainLabels_hot)  # starts training\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from keras import optimizers\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "#from keras.optimizers import Adam\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "#sgd = SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False)\n",
    "#model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "#annealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "callbacks_list = [lrate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size=64\n",
      "trainLabels_hot.shape=(3800, 12)\n",
      "steps_per_epoch=118\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch=int((len(trainData_normalize)*augRatio)/batch_size)\n",
    "validation_steps = int((len(valiData_normalize)*augRatio)/batch_size)\n",
    "\n",
    "print(\"batch_size={}\".format(batch_size))\n",
    "print(\"trainLabels_hot.shape={}\".format(trainLabels_hot.shape))\n",
    "print(\"steps_per_epoch={}\".format(steps_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "118/118 [==============================] - 25s - loss: 0.3036 - acc: 0.9086 - val_loss: 0.8167 - val_acc: 0.8679\n",
      "Epoch 2/100\n",
      "118/118 [==============================] - 24s - loss: 0.3314 - acc: 0.9204 - val_loss: 0.8357 - val_acc: 0.8729\n",
      "Epoch 3/100\n",
      "118/118 [==============================] - 24s - loss: 0.4379 - acc: 0.9207 - val_loss: 0.8738 - val_acc: 0.8785\n",
      "Epoch 4/100\n",
      "118/118 [==============================] - 24s - loss: 0.4654 - acc: 0.9252 - val_loss: 1.0455 - val_acc: 0.8954\n",
      "Epoch 5/100\n",
      "118/118 [==============================] - 24s - loss: 0.4500 - acc: 0.9312 - val_loss: 0.7496 - val_acc: 0.8882\n",
      "Epoch 6/100\n",
      "118/118 [==============================] - 24s - loss: 0.3632 - acc: 0.9358 - val_loss: 0.2645 - val_acc: 0.9231\n",
      "Epoch 7/100\n",
      "118/118 [==============================] - 24s - loss: 0.2283 - acc: 0.9439 - val_loss: 0.2228 - val_acc: 0.9326\n",
      "Epoch 8/100\n",
      "118/118 [==============================] - 24s - loss: 0.1494 - acc: 0.9536 - val_loss: 0.1725 - val_acc: 0.9348\n",
      "Epoch 9/100\n",
      "118/118 [==============================] - 24s - loss: 0.1151 - acc: 0.9605 - val_loss: 0.1092 - val_acc: 0.9582\n",
      "Epoch 10/100\n",
      "118/118 [==============================] - 24s - loss: 0.0935 - acc: 0.9668 - val_loss: 0.0881 - val_acc: 0.9647\n",
      "Epoch 11/100\n",
      "118/118 [==============================] - 24s - loss: 0.0846 - acc: 0.9693 - val_loss: 0.0933 - val_acc: 0.9644\n",
      "Epoch 12/100\n",
      "118/118 [==============================] - 24s - loss: 0.0758 - acc: 0.9722 - val_loss: 0.1001 - val_acc: 0.9632\n",
      "Epoch 13/100\n",
      "118/118 [==============================] - 24s - loss: 0.0678 - acc: 0.9736 - val_loss: 0.0868 - val_acc: 0.9665\n",
      "Epoch 14/100\n",
      "118/118 [==============================] - 24s - loss: 0.0652 - acc: 0.9756 - val_loss: 0.0862 - val_acc: 0.9679\n",
      "Epoch 15/100\n",
      "118/118 [==============================] - 24s - loss: 0.0601 - acc: 0.9768 - val_loss: 0.0863 - val_acc: 0.9658\n",
      "Epoch 16/100\n",
      "118/118 [==============================] - 24s - loss: 0.0597 - acc: 0.9778 - val_loss: 0.0776 - val_acc: 0.9691\n",
      "Epoch 17/100\n",
      "118/118 [==============================] - 24s - loss: 0.0554 - acc: 0.9795 - val_loss: 0.0902 - val_acc: 0.9640\n",
      "Epoch 18/100\n",
      "118/118 [==============================] - 24s - loss: 0.0563 - acc: 0.9785 - val_loss: 0.0867 - val_acc: 0.9661\n",
      "Epoch 19/100\n",
      "118/118 [==============================] - 24s - loss: 0.0535 - acc: 0.9792 - val_loss: 0.0714 - val_acc: 0.9716\n",
      "Epoch 20/100\n",
      "118/118 [==============================] - 24s - loss: 0.0473 - acc: 0.9816 - val_loss: 0.0740 - val_acc: 0.9702\n",
      "Epoch 21/100\n",
      "118/118 [==============================] - 24s - loss: 0.0474 - acc: 0.9821 - val_loss: 0.0755 - val_acc: 0.9702\n",
      "Epoch 22/100\n",
      "118/118 [==============================] - 24s - loss: 0.0475 - acc: 0.9821 - val_loss: 0.0780 - val_acc: 0.9692\n",
      "Epoch 23/100\n",
      "118/118 [==============================] - 24s - loss: 0.0424 - acc: 0.9833 - val_loss: 0.0753 - val_acc: 0.9708\n",
      "Epoch 24/100\n",
      "118/118 [==============================] - 24s - loss: 0.0461 - acc: 0.9826 - val_loss: 0.0717 - val_acc: 0.9724\n",
      "Epoch 25/100\n",
      "118/118 [==============================] - 24s - loss: 0.0407 - acc: 0.9841 - val_loss: 0.0630 - val_acc: 0.9751\n",
      "Epoch 26/100\n",
      "118/118 [==============================] - 24s - loss: 0.0403 - acc: 0.9846 - val_loss: 0.0703 - val_acc: 0.9736\n",
      "Epoch 27/100\n",
      "118/118 [==============================] - 24s - loss: 0.0392 - acc: 0.9846 - val_loss: 0.0704 - val_acc: 0.9709\n",
      "Epoch 28/100\n",
      "118/118 [==============================] - 24s - loss: 0.0393 - acc: 0.9850 - val_loss: 0.0719 - val_acc: 0.9718\n",
      "Epoch 29/100\n",
      "118/118 [==============================] - 24s - loss: 0.0385 - acc: 0.9847 - val_loss: 0.0720 - val_acc: 0.9728\n",
      "Epoch 30/100\n",
      "118/118 [==============================] - 24s - loss: 0.0371 - acc: 0.9856 - val_loss: 0.0712 - val_acc: 0.9717\n",
      "Epoch 31/100\n",
      "118/118 [==============================] - 24s - loss: 0.0359 - acc: 0.9863 - val_loss: 0.0668 - val_acc: 0.9732\n",
      "Epoch 32/100\n",
      "118/118 [==============================] - 24s - loss: 0.0345 - acc: 0.9867 - val_loss: 0.0671 - val_acc: 0.9738\n",
      "Epoch 33/100\n",
      "118/118 [==============================] - 24s - loss: 0.0343 - acc: 0.9865 - val_loss: 0.0648 - val_acc: 0.9735\n",
      "Epoch 34/100\n",
      "118/118 [==============================] - 24s - loss: 0.0346 - acc: 0.9869 - val_loss: 0.0692 - val_acc: 0.9731\n",
      "Epoch 35/100\n",
      "118/118 [==============================] - 24s - loss: 0.0338 - acc: 0.9872 - val_loss: 0.0648 - val_acc: 0.9742\n",
      "Epoch 36/100\n",
      "118/118 [==============================] - 24s - loss: 0.0337 - acc: 0.9874 - val_loss: 0.0665 - val_acc: 0.9735\n",
      "Epoch 37/100\n",
      "118/118 [==============================] - 24s - loss: 0.0330 - acc: 0.9874 - val_loss: 0.0658 - val_acc: 0.9743\n",
      "Epoch 38/100\n",
      "118/118 [==============================] - 24s - loss: 0.0347 - acc: 0.9860 - val_loss: 0.0627 - val_acc: 0.9749\n",
      "Epoch 39/100\n",
      "118/118 [==============================] - 24s - loss: 0.0335 - acc: 0.9870 - val_loss: 0.0669 - val_acc: 0.9729\n",
      "Epoch 40/100\n",
      "117/118 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9871"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-df14887ba404>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataAugment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     train_history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=epochs, \n\u001b[0;32m---> 12\u001b[0;31m                     validation_data=valdi_generator, validation_steps=validation_steps, callbacks=callbacks_list)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1122\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1922\u001b[0m                                 \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m                                 \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1924\u001b[0;31m                                 pickle_safe=pickle_safe)\n\u001b[0m\u001b[1;32m   1925\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1926\u001b[0m                             \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_q_size, workers, pickle_safe)\u001b[0m\n\u001b[1;32m   2019\u001b[0m                                      \u001b[0;34m'or (x, y). Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m                                      str(generator_output))\n\u001b[0;32m-> 2021\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[1;32m   1682\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1684\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1685\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#from keras.optimizers import SGD\n",
    "#model.compile(loss='binary_crossentropy', optimizer=SGD(lr=1e-4, momentum=0.9), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#sgd = SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False)\n",
    "#model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "#ADAM = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "ADAM = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=ADAM, metrics=['accuracy'])\n",
    "\n",
    "if(dataAugment):\n",
    "    #train_history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=epochs, \n",
    "    #                validation_data=valdi_generator, validation_steps=validation_steps, callbacks=callbacks_list)\n",
    "    train_history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=epochs, \n",
    "                    validation_data=valdi_generator, validation_steps=validation_steps)    \n",
    "    \n",
    "else:    \n",
    "    train_history = model.fit(x=trainData_normalize, y=trainLabels_hot, validation_split=ratioVal, \n",
    "                              epochs=epochs, batch_size=batch_size, verbose=1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_train_history(train_history, 'acc', 'val_acc')\n",
    "show_train_history(train_history, 'loss', 'val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_train_history(train_history, 'loss', 'val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(testImages_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "inv_dict_labels = {v: k for k, v in dict_labels.items()}\n",
    "\n",
    "results = np.argmax(Y_pred,axis = 1)\n",
    "testFiles = []\n",
    "testPredict = []\n",
    "i=0\n",
    "for filename in os.listdir(testDataset):\n",
    "    #print(\"{}) {}\".format(i, results[i]))\n",
    "    testFiles.append(filename)\n",
    "    testPredict.append(inv_dict_labels[results[i]])\n",
    "    i += 1\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "        \"file\": testFiles,\n",
    "        \"species\": testPredict\n",
    "    })\n",
    "submission.to_csv('Plant-submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
